{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    '''三目並べを実行する環境のクラス'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        num_states = 9\n",
    "        num_actions = 9\n",
    "        self.game_board = np.zeros(num_states)\n",
    "        self.agent = Agent(num_states, num_actions)\n",
    "        \n",
    "    \n",
    "    def run(self):\n",
    "        '''三目並べの実行'''\n",
    "        \n",
    "        win = 0 # 勝ち数\n",
    "        lose = 0 # 負け数\n",
    "        \n",
    "        for episode in range(NUM_EPISODES):\n",
    "            \n",
    "            self.game_board = np.zeros(9) # 盤面をリセット\n",
    "            observation = copy.deepcopy(self.game_board) # 盤面のコピー\n",
    "            reward = 0 # 報酬をリセット\n",
    "            winner = 0 # 勝利者をリセット\n",
    "            pre = random.randint(1, 2) # 1→Q学習が先攻、2→Q学習が後攻\n",
    "            \n",
    "            for step in range(5):\n",
    "                \n",
    "                if pre == 1:\n",
    "                    action, winner = self.q_learning_turn(observation, episode) # Q学習のターン\n",
    "                    if (winner == 0 and step < 4):\n",
    "                        pos, winner = self.random_player_turn() # ランダムプレイヤーのターン\n",
    "                else:\n",
    "                    pos, winner = self.random_player_turn() # ランダムプレイヤーのターン\n",
    "                    if (winner == 0 and step < 4):\n",
    "                        action, winner = self.q_learning_turn(observation, episode) # Q学習のターン\n",
    "                        \n",
    "                if winner == 1:\n",
    "                    win += 1\n",
    "                    reward = 1\n",
    "                elif winner == 2:\n",
    "                    lose += 1\n",
    "                    reward = -1\n",
    "                \n",
    "                if(step==4 and pre == 2):\n",
    "                    break\n",
    "                \n",
    "                observation_next = self.game_board # 両者が打ち込んだ後の状態\n",
    "                \n",
    "                self.agent.update_Q_function(observation, action, reward, observation_next) # Q関数の更新\n",
    "                \n",
    "                observation = copy.deepcopy(observation_next) # game_boardと一緒に更新されないようdeepcopyを使う\n",
    "                \n",
    "                if winner != 0 or step == 4:\n",
    "                    break\n",
    "        \n",
    "        print(\"win:\", win, \"lose:\", lose, \"draw\", NUM_EPISODES - win - lose)\n",
    "        print(\"勝率\", win / NUM_EPISODES)\n",
    "    \n",
    "    def q_learning_turn(self, observation, episode):\n",
    "        '''Q学習の行動選択、盤面反映、勝利判定'''\n",
    "        action = self.agent.get_action(observation, episode) # エージェントが行動を選択\n",
    "        self.game_board[action] = 1 # 盤面に反映\n",
    "        winner = self.win_func(1) # 勝っているか判定\n",
    "        return action, winner # 行動、勝利者を返す\n",
    "    \n",
    "    def random_player_turn(self):\n",
    "        '''ランダムプレイヤーの行動選択、盤面反映、勝利判定'''\n",
    "        pos = self.random_player_action() # ランダムプレイヤーが行動を選択\n",
    "        self.game_board[pos] = 2 # 盤面に反映\n",
    "        winner = self.win_func(2) # 勝っているか判定\n",
    "        return pos, winner # 行動、勝利者を返す\n",
    "    \n",
    "    def random_player_action(self):\n",
    "        '''空いているマスのランダムな行動選択'''\n",
    "        choices = []\n",
    "        for i in range(9):\n",
    "            if (self.game_board[i] == 0):\n",
    "                choices.append(i)\n",
    "        action = random.choice(choices)\n",
    "        return action\n",
    "        \n",
    "    def win_func(self, player):\n",
    "        '''勝利判定'''\n",
    "        # 勝ち手を列挙\n",
    "        lines = [\n",
    "          [0, 1, 2],\n",
    "          [3, 4, 5],\n",
    "          [6, 7, 8],\n",
    "          [0, 3, 6],\n",
    "          [1, 4, 7],\n",
    "          [2, 5, 8],\n",
    "          [0, 4, 8],\n",
    "          [2, 4, 6],\n",
    "        ]\n",
    "        for i in range(0, len(lines)):\n",
    "            [a, b, c] = lines[i]\n",
    "            if (self.game_board[a] and self.game_board[a] == self.game_board[b] and self.game_board[b] == self.game_board[c]):\n",
    "                return player # 勝ち手に当てはまってたら勝利者の番号を返す\n",
    "        return 0\n",
    "    \n",
    "    def show(self, action, player):\n",
    "        print(\"player:\", player, \", action:\", action)\n",
    "        board = np.reshape(self.game_board, [3,3])\n",
    "        print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''三目並べのエージェントクラス'''\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.brain = Brain(num_states, num_actions)\n",
    "        # エージェントが行動を決定するための頭脳を生成\n",
    "        \n",
    "    def update_Q_function(self, observation, action, reward, observation_next):\n",
    "        '''Q関数の更新'''\n",
    "        self.brain.update_Q_table(observation, action, reward, observation_next)\n",
    "            \n",
    "    def get_action(self, observation, episode):\n",
    "        '''行動の決定'''\n",
    "        action = self.brain.decide_action(observation, episode)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    '''エージェントが持つ脳となるクラスです。Q学習を実行します'''\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.count = 0\n",
    "        self.q_table = np.random.uniform(low=0, high=1, size=(3**num_states, num_actions))\n",
    "    \n",
    "    def digitize_state(self, observation):\n",
    "        return int(sum([x * (3**i) for i, x in enumerate(observation)]))\n",
    "    \n",
    "    def update_Q_table(self, observation, action, reward, observation_next):\n",
    "        '''QテーブルをQ学習より更新'''\n",
    "        state = self.digitize_state(observation) # 状態を離散化\n",
    "        state_next = self.digitize_state(observation_next) # 次の状態を離散化\n",
    "        Max_Q_next = max(self.q_table[state_next][:])\n",
    "        if (reward != 0):\n",
    "            self.q_table[state,action] = self.q_table[state, action] + ETA * (reward - self.q_table[state, action])\n",
    "        else:\n",
    "            self.q_table[state, action] = self.q_table[state, action] + ETA * (reward + GAMMA * Max_Q_next - self.q_table[state, action])\n",
    "        # 次の状態で選択できない行動の行動価値関数をゼロにしておく\n",
    "        choices = []\n",
    "        for i in range(9):\n",
    "            if (observation_next[i] != 0):\n",
    "                choices.append(i)\n",
    "        for x in choices:\n",
    "            self.q_table[state_next][x] = MINIMUM\n",
    "        \n",
    "    def decide_action(self, observation, episode):\n",
    "        '''ε-greedy法で徐々に最適行動のみを採用する'''\n",
    "        \n",
    "        state = self.digitize_state(observation)\n",
    "        \n",
    "        epsilon = 0.5 * (1 / (episode + 1))\n",
    "\n",
    "        choices = []\n",
    "        for i in range(9):\n",
    "            if (observation[i] == 0):\n",
    "                choices.append(i)\n",
    "                    \n",
    "        if epsilon <= np.random.uniform(0, 1):\n",
    "            action = np.argmax(self.q_table[state][:])\n",
    "            if (action not in choices):\n",
    "                action = random.choice(choices)\n",
    "            return action\n",
    "        else:\n",
    "            choices = []\n",
    "            for i in range(9):\n",
    "                if (observation[i] == 0):\n",
    "                    choices.append(i)\n",
    "            action = random.choice(choices)\n",
    "            return action\n",
    "                \n",
    "    def show(self, observation):\n",
    "        board = np.reshape(observation, [3,3])\n",
    "        print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "win: 869999 lose: 95972 draw 34029\n",
      "勝率 0.869999\n"
     ]
    }
   ],
   "source": [
    "NUM_EPISODES = 1000000\n",
    "COMPLETE_EPISODES = 100\n",
    "MINIMUM = - 10 ** 10\n",
    "GAMMA = 0.99 # 時間割引率\n",
    "ETA = 0.5 # 学習係数\n",
    "env = Environment()\n",
    "env.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VS_Q_Learning:\n",
    "    \n",
    "    def __init__(self):\n",
    "        num_states = 9\n",
    "        num_actions = 9\n",
    "        self.game_board = np.zeros(num_states)\n",
    "        self.agent = VS_Agent(num_states, num_actions)\n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        pre = int(input(\"先攻→0, 後攻→1を入力してください\"))\n",
    "        print(\"ゲームスタート！\")\n",
    "        self.show()\n",
    "        observation = copy.deepcopy(self.game_board)\n",
    "        winner = 0\n",
    "            \n",
    "        for step in range(9):\n",
    "            \n",
    "            if pre == 0:\n",
    "                winner = self.your_turn()\n",
    "                if (winner == 0 and step < 4):\n",
    "                    winner = self.q_learning_turn()\n",
    "            else:\n",
    "                winner = self.q_learning_turn()\n",
    "                if (winner == 0 and step < 4):\n",
    "                    winner = self.your_turn()\n",
    "\n",
    "            if winner == 1:\n",
    "                print(\"AIの勝ち\")\n",
    "                break\n",
    "            elif winner == 2:\n",
    "                print(\"あなたの勝ち\")\n",
    "                break\n",
    "\n",
    "            if step == 4:\n",
    "                print(\"引き分け\")\n",
    "                complete_episodes = 0\n",
    "                break\n",
    "    \n",
    "    def your_turn(self):\n",
    "        print(\"あなたのターン\")\n",
    "        pos = int(input(\"手を選んでください:\"))\n",
    "        self.game_board[pos] = 2\n",
    "        self.show()\n",
    "        winner = self.winner_func(2)\n",
    "        return winner\n",
    "            \n",
    "    def q_learning_turn(self):\n",
    "        print(\"AIのターン\")\n",
    "        action = self.agent.get_action(self.game_board)\n",
    "        self.game_board[action] = 1\n",
    "        self.show()\n",
    "        winner = self.winner_func(1)\n",
    "        return winner\n",
    "    \n",
    "    def winner_func(self, player):\n",
    "        # 勝ち手を列挙\n",
    "        lines = [\n",
    "          [0, 1, 2],\n",
    "          [3, 4, 5],\n",
    "          [6, 7, 8],\n",
    "          [0, 3, 6],\n",
    "          [1, 4, 7],\n",
    "          [2, 5, 8],\n",
    "          [0, 4, 8],\n",
    "          [2, 4, 6],\n",
    "        ]\n",
    "        for i in range(0, len(lines)):\n",
    "            [a, b, c] = lines[i]\n",
    "            \n",
    "            if self.game_board[a] and self.game_board[a] == self.game_board[b] and self.game_board[a] == self.game_board[c]: \n",
    "                return player\n",
    "        return 0\n",
    "    \n",
    "    def show(self):\n",
    "        show_board = np.array([\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"])\n",
    "        for i in range(9):\n",
    "            x = self.game_board[i]\n",
    "            if (x==0):\n",
    "                show_board[i] = str(i)\n",
    "            elif (x==1):\n",
    "                show_board[i] = \"×\"\n",
    "            else:\n",
    "                show_board[i] = \"⚪︎\"\n",
    "        board = np.reshape(show_board, [3,3])\n",
    "        print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VS_Agent:\n",
    "    '''三目並べのエージェントクラス'''\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.brain = VS_Brain(num_states, num_actions)\n",
    "        # エージェントが行動を決定するための頭脳を生成\n",
    "            \n",
    "    def get_action(self, observation):\n",
    "        '''行動の決定'''\n",
    "        action = self.brain.decide_action(observation)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VS_Brain:\n",
    "    '''エージェントが持つ脳となるクラスです。Q学習を実行します'''\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def digitize_state(self, observation):\n",
    "        return int(sum([x * (3**i) for i, x in enumerate(observation)]))\n",
    "        \n",
    "    def decide_action(self, observation):\n",
    "        choices = []\n",
    "        for i in range(9):\n",
    "            if (observation[i] == 0):\n",
    "                choices.append(i)\n",
    "        state = self.digitize_state(observation)\n",
    "        action = np.argmax(q_table[state][:])\n",
    "        if (action not in choices):\n",
    "            action = random.choice(choices)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "先攻→0, 後攻→1を入力してください1\n",
      "ゲームスタート！\n",
      "[['0' '1' '2']\n",
      " ['3' '4' '5']\n",
      " ['6' '7' '8']]\n",
      "AIのターン\n",
      "[['0' '1' '2']\n",
      " ['3' '×' '5']\n",
      " ['6' '7' '8']]\n",
      "あなたのターン\n",
      "手を選んでください:1\n",
      "[['0' '⚪' '2']\n",
      " ['3' '×' '5']\n",
      " ['6' '7' '8']]\n",
      "AIのターン\n",
      "[['×' '⚪' '2']\n",
      " ['3' '×' '5']\n",
      " ['6' '7' '8']]\n",
      "あなたのターン\n",
      "手を選んでください:8\n",
      "[['×' '⚪' '2']\n",
      " ['3' '×' '5']\n",
      " ['6' '7' '⚪']]\n",
      "AIのターン\n",
      "[['×' '⚪' '2']\n",
      " ['×' '×' '5']\n",
      " ['6' '7' '⚪']]\n",
      "あなたのターン\n",
      "手を選んでください:6\n",
      "[['×' '⚪' '2']\n",
      " ['×' '×' '5']\n",
      " ['⚪' '7' '⚪']]\n",
      "AIのターン\n",
      "[['×' '⚪' '2']\n",
      " ['×' '×' '×']\n",
      " ['⚪' '7' '⚪']]\n",
      "AIの勝ち\n"
     ]
    }
   ],
   "source": [
    "q_table = env.agent.brain.q_table\n",
    "game = VS_Q_Learning()\n",
    "game.play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
